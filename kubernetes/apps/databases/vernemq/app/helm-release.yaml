---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: vernemq
spec:
  interval: 1h
  chartRef:
    kind: OCIRepository
    name: app-template
  values:
    controllers:
      vernemq:
        type: statefulset
        strategy: RollingUpdate
        replicas: 3
        annotations:
          secret.reloader.stakater.com/reload: vernemq-auth-secret
        pod:
          securityContext:
            runAsUser: 10000
            runAsNonRoot: true

        statefulset:
          serviceName:
            identifier: headless

        containers:
          app:
            image:
              repository: vernemq/vernemq
              tag: 2.1.1-alpine
            env:
              MY_POD_NAME:
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              DOCKER_VERNEMQ_ACCEPT_EULA: "yes"
              DOCKER_VERNEMQ_DISCOVERY_KUBERNETES: 1
              DOCKER_VERNEMQ_KUBERNETES_LABEL_SELECTOR: "app.kubernetes.io/name=vernemq"
              DOCKER_VERNEMQ_LEVELDB__MAXIMUM_MEMORY: "128000000"
              DOCKER_VERNEMQ_LEVELDB__WRITE_BUFFER_SIZE_MIN: "2500000"
              DOCKER_VERNEMQ_LEVELDB__WRITE_BUFFER_SIZE_MAX: "7500000"
              # allow clients to failover when pods are restarting
              DOCKER_VERNEMQ_ALLOW_REGISTER_DURING_NETSPLIT: "on"
              DOCKER_VERNEMQ_ALLOW_PUBLISH_DURING_NETSPLIT: "on"
              DOCKER_VERNEMQ_ALLOW_SUBSCRIBE_DURING_NETSPLIT: "on"
              DOCKER_VERNEMQ_ALLOW_UNSUBSCRIBE_DURING_NETSPLIT: "on"

              # Kubernetes namespace (required for discovery)
              DOCKER_VERNEMQ_KUBERNETES_NAMESPACE: "databases"
              # Kubernetes service account and RBAC permissions
              DOCKER_VERNEMQ_KUBERNETES_USE_LONGNAMES: "true"
              # Cluster formation settings
              DOCKER_VERNEMQ_NODENAME: "VerneMQ@$(MY_POD_NAME).vernemq-headless.databases.svc.cluster.local"
              # This should force cleanup of dead nodes
              #DOCKER_VERNEMQ_CLUSTER_LEAVE_NODE_ON_UNSUBSCRIBE: "on"
              #DOCKER_VERNEMQ_CLUSTER_TRADE_CONSISTENCY: "on"
              # Add this to handle node removal during updates
              #DOCKER_VERNEMQ_CLUSTER_PURGE_OFFLINE_NODES_AFTER: "60000" # 60 seconds
              # Increase clustering timeouts
              #DOCKER_VERNEMQ_CLUSTER_FORMATION_TIMEOUT: "120000" # 2 minutes
              #DOCKER_VERNEMQ_CLUSTER_JOIN_TIMEOUT: "60000" # 1 minute
              # Add Erlang distribution settings
              #DOCKER_VERNEMQ_ERLANG__DISTRIBUTION__PORT_RANGE__MINIMUM: "44053"
              #DOCKER_VERNEMQ_ERLANG__DISTRIBUTION__PORT_RANGE__MAXIMUM: "44053"
              DOCKER_VERNEMQ_LOG__CONSOLE__LEVEL: "debug"

            envFrom:
              - secretRef:
                  name: vernemq-auth-secret
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health/ping
                    port: 8888
                  periodSeconds: 10
                  timeoutSeconds: 5
              readiness: *probes
              startup:
                <<: *probes
                spec:
                  exec:
                    command:
                      # vernemq immediately reports healthy, even before retained messages are replicated
                      # so we need to make sure all nodes have the same num_replicated on startup
                      # see https://github.com/vernemq/docker-vernemq/issues/255
                      - /bin/sh
                      - -c
                      - >-
                        curl -s localhost:8888/status.json |
                        jq -e '.[0] | [to_entries.[].value.num_retained] | .[0] as $x | all(.[]; . == $x)'
                  periodSeconds: 10
                  timeoutSeconds: 5
            resources:
              requests:
                cpu: 20m
                memory: 128Mi
              limits:
                memory: 512Mi
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                  - ALL

    service:
      app:
        primary: true
        type: LoadBalancer
        annotations:
          lbipam.cilium.io/ips: 192.168.20.213
        ports:
          mqtt:
            port: 1883
          ws:
            port: 8080
      headless:
        publishNotReadyAddresses: true
        clusterIP: None
        ports:
          mqtt:
            port: 1883
          ws:
            port: 8080
      metrics:
        ports:
          http:
            port: 8888

    serviceMonitor:
      app:
        service:
          identifier: metrics
        endpoints:
          - port: http
            scheme: http
            path: /metrics
            interval: 10s
            scrapeTimeout: 1s

    persistence:
      data:
        type: emptyDir
        globalMounts:
          - path: /vernemq/data
      tmpfs:
        type: emptyDir
        advancedMounts:
          vernemq:
            app:
              - path: /vernemq/log
                subPath: log
              - path: /tmp
                subPath: tmp

    serviceAccount:
      vernemq: {}

    rbac:
      roles:
        vernemq:
          type: Role
          rules:
            - apiGroups:
                - ""
              resources:
                - pods
              verbs:
                - get
                - list
            - apiGroups:
                - apps
              resources:
                - statefulsets
              verbs:
                - get
      bindings:
        vernemq:
          type: RoleBinding
          roleRef:
            identifier: vernemq
          subjects:
            - identifier: vernemq
